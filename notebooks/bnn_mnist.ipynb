{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning posteriors for BNN on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchmetrics tqdm pandas matplotlib\n",
    "# !pip install \"numpy<2.0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics.functional.classification import multiclass_calibration_error\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to import NormFlows: No module named 'normflows'\n",
      "WARNING:root:Failed to import NormFlows: No module named 'normflows'\n"
     ]
    }
   ],
   "source": [
    "import reparameterized\n",
    "from reparameterized import sampling\n",
    "\n",
    "from reparameterized.likelihoods import (\n",
    "    categorical_posterior_probs as posterior_predictions,\n",
    ")\n",
    "from reparameterized.likelihoods import categorical_log_prob\n",
    "from reparameterized.bnn_wrapper import elbo_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ipykernel\" in sys.argv[0]:\n",
    "    sys.argv = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--wandb'], dest='wandb', nargs=0, const=True, default=False, type=None, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Reparameterized: BNN+MNIST\")\n",
    "\n",
    "parser.add_argument(\"--name\", type=str, default=None)\n",
    "\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"mnist\", choices=(\"mnist\"))\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1024)\n",
    "\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"Adam\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "parser.add_argument(\"--n_posterior_samples\", type=int, default=17)\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=100)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--posterior_model_architecture\",\n",
    "    type=str,\n",
    "    default=\"svd_small_rnvp_rezero_on_residuals\",\n",
    "    choices=(\n",
    "        \"rnvp_rezero\",\n",
    "        \"rnvp\",\n",
    "        \"rnvp_rezero_small\",\n",
    "        \"rnvp_small\",\n",
    "        \"factorized_gaussian\",\n",
    "        \"factorized_gaussian_rezero\",\n",
    "        \"gaussian_tril\",\n",
    "        \"gaussian_tril_rezero\",\n",
    "        \"gaussian_full\",\n",
    "        \"gaussian_full_rezero\",\n",
    "        \"gaussian_lowrank\",        \n",
    "        \"gaussian_lowrank_rezero\",        \n",
    "        \"svd_rnvp\",\n",
    "        \"svd_rnvp_rezero\",\n",
    "        \"svd_rnvp_rezero_on_residuals\",\n",
    "        \"svd_rnvp_small\",\n",
    "        \"svd_rnvp_small_rezero\",\n",
    "        \"svd_rnvp_small_rezero_on_residuals\",\n",
    "        \"svd_gaussian_lowrank\",\n",
    "        \"svd_factorized_gaussian\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--distributional_parameters\", type=str, nargs=\"+\", default=[\"last_layer.weight\", \"common_layers.2.0\"]\n",
    ")\n",
    "parser.add_argument(\"--joint_sampling\", default=True, action=\"store_false\")\n",
    "\n",
    "parser.add_argument(\"--seed\", type=int, default=1863)\n",
    "parser.add_argument(\"--wandb\", default=False, action=\"store_true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = parser.parse_args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.name is None:\n",
    "    distributional_parameters_str = \"_\".join(cfg.distributional_parameters)\n",
    "    cfg.name = f\"{cfg.dataset}_{cfg.posterior_model_architecture}_seed{cfg.seed}_N{cfg.n_posterior_samples}_E{cfg.n_epochs}_lr{cfg.lr}_P{distributional_parameters_str}_{cfg.optimizer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_log(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "if cfg.wandb:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(project=\"Reparameterized: BNN+MNIST\", config=cfg.__dict__, name=cfg.name)\n",
    "\n",
    "    def wandb_log(*args, **kwargs):\n",
    "        wandb.log(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg = Namespace(name='mnist_svd_small_rnvp_rezero_on_residuals_seed1863_N17_E100_lr0.001_Plast_layer.weight_common_layers.2.0_Adam', dataset='mnist', batch_size=1024, optimizer='Adam', lr=0.001, n_posterior_samples=17, n_epochs=100, posterior_model_architecture='svd_small_rnvp_rezero_on_residuals', distributional_parameters=['last_layer.weight', 'common_layers.2.0'], joint_sampling=True, seed=1863, wandb=False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"cfg = {cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataloaders(batch_size=128):\n",
    "    mnist_train_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    mnist_test_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor(),\n",
    "    )\n",
    "\n",
    "    mnist_ood_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        mnist_train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    dataloader_test = DataLoader(\n",
    "        mnist_test_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    dataloader_ood = DataLoader(mnist_ood_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return dataloader_train, dataloader_test, dataloader_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNNMnist(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=784,\n",
    "        out_dim=10,\n",
    "        hid_dim=128,\n",
    "        num_layers=2,\n",
    "        device=torch.device(\"cuda\"),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.common_layers = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, self.hid_dim),\n",
    "            *[\n",
    "                nn.Sequential(nn.Linear(self.hid_dim, self.hid_dim), nn.ELU())\n",
    "                for _ in range(self.num_layers)\n",
    "            ],\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.last_layer = nn.Linear(self.hid_dim, self.out_dim).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.common_layers(x)\n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.dataset == \"mnist\":\n",
    "    train_dataloader, test_dataloader, _ = get_mnist_dataloaders(\n",
    "        batch_size=cfg.batch_size\n",
    "    )\n",
    "    bnn = BNNMnist(in_dim=784, out_dim=10, hid_dim=128, num_layers=2, device=device)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Dataset={cfg.dataset} not supported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors and Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      " - common_layers.0.weight: torch.Size([128, 784]) (grad=True) (learn distribution=False)\n",
      " - common_layers.0.bias: torch.Size([128]) (grad=True) (learn distribution=False)\n",
      " - common_layers.1.0.weight: torch.Size([128, 128]) (grad=True) (learn distribution=False)\n",
      " - common_layers.1.0.bias: torch.Size([128]) (grad=True) (learn distribution=False)\n",
      " - common_layers.2.0.weight: torch.Size([128, 128]) (grad=True) (learn distribution=True)\n",
      " - common_layers.2.0.bias: torch.Size([128]) (grad=True) (learn distribution=True)\n",
      " - last_layer.weight: torch.Size([10, 128]) (grad=True) (learn distribution=True)\n",
      " - last_layer.bias: torch.Size([10]) (grad=True) (learn distribution=False)\n"
     ]
    }
   ],
   "source": [
    "pointwise_params = {}\n",
    "distributional_params = {}\n",
    "\n",
    "print(\"Model parameters:\")\n",
    "for n, p in bnn.named_parameters():\n",
    "\n",
    "    distributional = any(\n",
    "        (param_selector in n) for param_selector in cfg.distributional_parameters\n",
    "    )\n",
    "\n",
    "    print(f\" - {n}: {p.shape} (grad={p.requires_grad}) (learn distribution={distributional})\")\n",
    "\n",
    "    if distributional:\n",
    "        distributional_params[n] = p\n",
    "    else:\n",
    "        pointwise_params[n] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_basic_prior(p):\n",
    "    p = p.flatten()\n",
    "    return torch.distributions.MultivariateNormal(\n",
    "        loc=torch.zeros_like(p), covariance_matrix=torch.diag(torch.ones_like(p))\n",
    "    )\n",
    "\n",
    "\n",
    "priors = {n: create_gaussian_basic_prior(p) for n, p in distributional_params.items()}\n",
    "\n",
    "\n",
    "def log_priors(samples):\n",
    "    return sum(\n",
    "        priors[n].log_prob(p.flatten()) for n, p in samples.items()\n",
    "    )  # sum over all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed_all(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[create_multiparameter_svd_sampler] Failed creating SVD projection for parameter=common_layers.2.0.bias. Falling back to sampling directly with <function create_flow_sampler at 0x12e43a660>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters are put together and use a joint sampler\n",
      "Posterior=svd_small_rnvp_rezero_on_residuals:\n",
      " - target_params = ['common_layers.2.0.weight', 'common_layers.2.0.bias', 'last_layer.weight']\n",
      " - sampler = <function create_multiparameter_svd_sampler.<locals>.sampler at 0x12e47e200>\n",
      " - variational_params = ['alpha', 'beta', 't.0.0.weight', 't.0.0.bias', 't.0.2.weight', 't.0.2.bias', 't.0.4.weight', 't.0.4.bias', 't.1.0.weight', 't.1.0.bias', 't.1.2.weight', 't.1.2.bias', 't.1.4.weight', 't.1.4.bias', 't.2.0.weight', 't.2.0.bias', 't.2.2.weight', 't.2.2.bias', 't.2.4.weight', 't.2.4.bias', 't.3.0.weight', 't.3.0.bias', 't.3.2.weight', 't.3.2.bias', 't.3.4.weight', 't.3.4.bias', 't.4.0.weight', 't.4.0.bias', 't.4.2.weight', 't.4.2.bias', 't.4.4.weight', 't.4.4.bias', 't.5.0.weight', 't.5.0.bias', 't.5.2.weight', 't.5.2.bias', 't.5.4.weight', 't.5.4.bias', 't.6.0.weight', 't.6.0.bias', 't.6.2.weight', 't.6.2.bias', 't.6.4.weight', 't.6.4.bias', 't.7.0.weight', 't.7.0.bias', 't.7.2.weight', 't.7.2.bias', 't.7.4.weight', 't.7.4.bias', 's.0.0.weight', 's.0.0.bias', 's.0.2.weight', 's.0.2.bias', 's.0.4.weight', 's.0.4.bias', 's.1.0.weight', 's.1.0.bias', 's.1.2.weight', 's.1.2.bias', 's.1.4.weight', 's.1.4.bias', 's.2.0.weight', 's.2.0.bias', 's.2.2.weight', 's.2.2.bias', 's.2.4.weight', 's.2.4.bias', 's.3.0.weight', 's.3.0.bias', 's.3.2.weight', 's.3.2.bias', 's.3.4.weight', 's.3.4.bias', 's.4.0.weight', 's.4.0.bias', 's.4.2.weight', 's.4.2.bias', 's.4.4.weight', 's.4.4.bias', 's.5.0.weight', 's.5.0.bias', 's.5.2.weight', 's.5.2.bias', 's.5.4.weight', 's.5.4.bias', 's.6.0.weight', 's.6.0.bias', 's.6.2.weight', 's.6.2.bias', 's.6.4.weight', 's.6.4.bias', 's.7.0.weight', 's.7.0.bias', 's.7.2.weight', 's.7.2.bias', 's.7.4.weight', 's.7.4.bias']\n",
      " - aux_objs = ['common_layers.2.0.weight.u', 'common_layers.2.0.weight.s', 'common_layers.2.0.weight.vh', 'last_layer.weight.u', 'last_layer.weight.s', 'last_layer.weight.vh', 'flow']\n"
     ]
    }
   ],
   "source": [
    "if not cfg.joint_sampling:\n",
    "    print(\"Each parameter gets its own sampler\")\n",
    "    parameter2sampler, variational_params, aux_objs = (\n",
    "        sampling.create_independent_samplers(\n",
    "            distributional_params, cfg.posterior_model_architecture\n",
    "        )\n",
    "    )\n",
    "    sampler = reparameterized.parameter_samplers_to_joint_sampler(parameter2sampler)\n",
    "\n",
    "else:\n",
    "    print(\"All parameters are put together and use a joint sampler\")\n",
    "    sampler, variational_params, aux_objs = sampling.create_joint_sampler(\n",
    "        distributional_params, cfg.posterior_model_architecture\n",
    "    )\n",
    "\n",
    "print(f\"Posterior={cfg.posterior_model_architecture}:\")\n",
    "print(f\" - target_params = {list(distributional_params.keys())}\")\n",
    "print(f\" - sampler = {sampler}\")\n",
    "print(f\" - variational_params = {list(variational_params.keys())}\")\n",
    "print(f\" - aux_objs = {list(aux_objs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_posterior(bnn, samples, dataloader, name_preffix=\"\", device=device):\n",
    "    metrics = dict()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    for step_no, (x, y) in enumerate(dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        probs = posterior_predictions(bnn, x, samples=samples)\n",
    "        probs = torch.mean(probs, dim=0)\n",
    "        class_predictions = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        all_targets.append(y)\n",
    "        all_predictions.append(class_predictions)\n",
    "        all_probabilities.append(probs)\n",
    "\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    all_probabilities = torch.cat(all_probabilities)\n",
    "\n",
    "    accuracy = torch.sum(all_predictions == all_targets) / len(all_targets)\n",
    "\n",
    "    ece_l1 = multiclass_calibration_error(\n",
    "        preds=all_probabilities,\n",
    "        target=all_targets,\n",
    "        num_classes=10,\n",
    "        n_bins=15,\n",
    "        norm=\"l1\",\n",
    "    )\n",
    "    ece_l2 = multiclass_calibration_error(\n",
    "        preds=all_probabilities,\n",
    "        target=all_targets,\n",
    "        num_classes=10,\n",
    "        n_bins=15,\n",
    "        norm=\"l2\",\n",
    "    )\n",
    "    ece_max = multiclass_calibration_error(\n",
    "        preds=all_probabilities,\n",
    "        target=all_targets,\n",
    "        num_classes=10,\n",
    "        n_bins=15,\n",
    "        norm=\"max\",\n",
    "    )\n",
    "\n",
    "    metrics[f\"{name_preffix}eval_accuracy\"] = accuracy\n",
    "    metrics[f\"{name_preffix}eval_ece_l1\"] = ece_l1\n",
    "    metrics[f\"{name_preffix}eval_ece_l2\"] = ece_l2\n",
    "    metrics[f\"{name_preffix}eval_ece_max\"] = ece_max\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate before training\n",
      "[start] metrics=eval_accuracy=0.11 eval_ece_l1=0.04 eval_ece_l2=0.04 eval_ece_max=0.20\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate before training\")\n",
    "\n",
    "samples, q_nlls = sampler(n_samples=111)\n",
    "assert not q_nlls.isnan().any(), q_nlls\n",
    "\n",
    "metrics = eval_posterior(bnn, samples, test_dataloader, device=device)\n",
    "metrics_str = \" \".join([f\"{k}={v:.2f}\" for k, v in metrics.items()])\n",
    "print(f\"[start] metrics={metrics_str}\")\n",
    "wandb_log(metrics, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed_all(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#variational params = 34420528\n"
     ]
    }
   ],
   "source": [
    "total_variational_params = sum(p.numel() for p in variational_params.values())\n",
    "print(f\"#variational params = {total_variational_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters = list(variational_params.values()) + list(\n",
    "    pointwise_params.values()\n",
    ")\n",
    "optimizer = getattr(torch.optim, cfg.optimizer)(optimized_parameters, lr=cfg.lr)\n",
    "\n",
    "omega = 1.0\n",
    "beta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:34, 94.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m full2minibatch_ratio = \u001b[38;5;28mlen\u001b[39m(train_dataloader.dataset) / \u001b[38;5;28mlen\u001b[39m(x)\n\u001b[32m     10\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m elbo_res = \u001b[43melbo_mc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_priors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_log_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_posterior_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull2minibatch_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m log_lik, KLD = elbo_res[\u001b[33m\"\u001b[39m\u001b[33mll\u001b[39m\u001b[33m\"\u001b[39m], elbo_res[\u001b[33m\"\u001b[39m\u001b[33mkl\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m loss_vi = -(omega * log_lik - beta * KLD)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/notebooks/../reparameterized/bnn_wrapper.py:177\u001b[39m, in \u001b[36melbo_mc\u001b[39m\u001b[34m(network, minibatch_x, minibatch_y, log_priors, log_likelihood, sampler, n_posterior_samples, full2minibatch_ratio, **sampler_kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34melbo_mc\u001b[39m(\n\u001b[32m    151\u001b[39m     network,\n\u001b[32m    152\u001b[39m     minibatch_x,\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m     **sampler_kwargs,\n\u001b[32m    160\u001b[39m ):\n\u001b[32m    161\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Computes the Monte-Carlo estimate of the Evidence Lower Bound (ELBO) for a Bayesian Neural Network (BNN).\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m \u001b[33;03m            the log likelihood. Defaults to 1.0.\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     samples, q_nlls = \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_posterior_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msampler_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m q_nlls.isnan().any(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed sampling! NLLs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_nlls\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m     p_nlls = [-log_priors(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m take_parameters_sample(samples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/notebooks/../reparameterized/sampling/svd.py:146\u001b[39m, in \u001b[36mcreate_multiparameter_svd_sampler.<locals>.sampler\u001b[39m\u001b[34m(n_samples, **inner_sampler_kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msampler\u001b[39m(n_samples=\u001b[32m1\u001b[39m, **inner_sampler_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     samples, nlls = \u001b[43mmultiparameter_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minner_sampler_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, r_samples \u001b[38;5;129;01min\u001b[39;00m samples.items():\n\u001b[32m    149\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (name + \u001b[33m\"\u001b[39m\u001b[33m.u\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m aux_objs:  \u001b[38;5;66;03m# SVD projection\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/notebooks/../reparameterized/sampling/multiparameter.py:127\u001b[39m, in \u001b[36mcreate_multiparameter_sampler_dict.<locals>._sampler_dict_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sampler_dict_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Matches unnamed samples from sampler with their names.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     parameter_samples, joint_nlls = \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(parameter_names, parameter_samples)), joint_nlls\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/notebooks/../reparameterized/sampling/multiparameter.py:87\u001b[39m, in \u001b[36mcreate_multiparameter_sampler.<locals>._sampler_list_wrapper\u001b[39m\u001b[34m(n_samples)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sampler_list_wrapper\u001b[39m(n_samples=\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     joint_samples, joint_nlls = \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     89\u001b[39m         joint_samples.shape[\u001b[32m0\u001b[39m] == n_samples\n\u001b[32m     90\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msampler=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampler\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m returned wrong no of samples\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m     samples = separate_parameters(joint_samples, named_parameters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/notebooks/../reparameterized/sampling/flows.py:36\u001b[39m, in \u001b[36mcreate_flow_sampler.<locals>.sample\u001b[39m\u001b[34m(n_samples)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(n_samples=\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     sample, nll = \u001b[43mflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_nll\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     sample = sample.reshape(n_samples, *parameter.size())\n\u001b[32m     39\u001b[39m     nll = nll.to(sample.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/notebooks/../reparameterized/sampling/realnvp.py:67\u001b[39m, in \u001b[36mRealNVP.sample\u001b[39m\u001b[34m(self, batchSize, D, calculate_nll)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batchSize, D, calculate_nll=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprior\u001b[49m.sample((batchSize,))\n\u001b[32m     68\u001b[39m     x, log_det_J = \u001b[38;5;28mself\u001b[39m.f_inv(z)\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m calculate_nll:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/notebooks/../reparameterized/sampling/realnvp.py:33\u001b[39m, in \u001b[36mRealNVP.prior\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprior\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprior_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprior_cov\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prj/reparametrized_pytorch/.env/lib/python3.12/site-packages/torch/distributions/multivariate_normal.py:182\u001b[39m, in \u001b[36mMultivariateNormal.__init__\u001b[39m\u001b[34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[39m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28mself\u001b[39m._unbroadcasted_scale_tril = scale_tril\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m covariance_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28mself\u001b[39m._unbroadcasted_scale_tril = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# precision_matrix is not None\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mself\u001b[39m._unbroadcasted_scale_tril = _precision_to_scale_tril(precision_matrix)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(cfg.n_epochs):\n",
    "\n",
    "    loss_vi_k = []\n",
    "    KLD_k = []\n",
    "    log_lik_k = []\n",
    "    for it, (x, y) in tqdm.tqdm(enumerate(train_dataloader)):\n",
    "        full2minibatch_ratio = len(train_dataloader.dataset) / len(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        elbo_res = elbo_mc(\n",
    "            bnn,\n",
    "            x,\n",
    "            y,\n",
    "            log_priors,\n",
    "            categorical_log_prob,\n",
    "            sampler,\n",
    "            cfg.n_posterior_samples,\n",
    "            full2minibatch_ratio,\n",
    "        )\n",
    "        log_lik, KLD = elbo_res[\"ll\"], elbo_res[\"kl\"]\n",
    "        loss_vi = -(omega * log_lik - beta * KLD)\n",
    "        loss_vi.backward()\n",
    "\n",
    "        loss_vi_k.append(loss_vi.detach().cpu().item())\n",
    "        KLD_k.append(KLD.detach().cpu().item())\n",
    "        log_lik_k.append(log_lik.detach().cpu().item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # reporting\n",
    "    log_lik = np.mean(log_lik_k)\n",
    "    KLD = np.mean(KLD_k)\n",
    "    loss_vi = np.mean(loss_vi_k)\n",
    "\n",
    "    samples, q_nlls = sampler(n_samples=111)\n",
    "    assert not q_nlls.isnan().any(), q_nlls\n",
    "\n",
    "    metrics = eval_posterior(bnn, samples, test_dataloader, device=device)\n",
    "    metrics_str = \" \".join([f\"{k}={v:.3f}\" for k, v in metrics.items()])\n",
    "    wandb_log(metrics, step=epoch)\n",
    "\n",
    "    res_str = f\"train: loss={loss_vi:.2f} log_lik={log_lik:.2f} KLD={KLD:.2f}\"\n",
    "    print(\n",
    "        f\"[{time.time()-start_time:.0f}s][epoch={epoch}] {res_str}  / test metrics: {metrics_str}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
